\section{Evolutionary Artificial Neural Network}
\subsection{General neural network}

In this paper, the feedforward ANN is adopted in the current study, since it is
straightforward to code. For function approximation through an artificial neural
network, Cybenko demonstrated that a two-layer perceptron can form an
arbitrarily close approximation to any continuous nonlinear mapping\cite{cybenko1989approximation}.
Therefore, a two-layer feedforward ANN is proposed in the present study. Fig. \ref{fig:gnn}
shows a general framework for a two-layer ANN, in which the number of nodes in
the hidden layer and the connection with inputs, are critical in the design of
an ANN. The nodes in the hidden layer are treated as feature extractors or
detectors. Therefore, nodes within this layer should partially be connected with
the inputs of an ANN, since the unnecessary connections would increase the
model’s complicacy, which will reduce an ANN’s performance. The number of nodes
in the hidden layer should be less than the number of inputs since the nodes in
the hidden layer are features. For the nodes in the last layer, every node
should be fully connected with nodes in the previous layer, the relationship
between the outputs and features should be direct. The rest, which affects the
performance of an artificial neural network, are the activation function, and
ANNs training method. In the following section, the $i$th node in the
input layer is denoted as $i_i$, and the $j$th node in the hidden layerd denoted
as $h_j$, respectively.

\input{fig/ann_general_architecture}
\subsection{Activation function}

The activation function is one of the critical parts of an ANN. Liu
\cite{liu1996evolutionary} et al. claims that the performance of neural networks with
different activation functions is different, even if they have the same
architecture.  A generalized activation function can be written as

\begin{equation}
	y_i = f_i(\sum_{j=1}^n{w_{ij}x_j - \theta})
\end{equation}

where $y_i$ is the output of the node $i$, $x_j$ is the $j$th input to the
node, and $w_{ij}$ is the connection weight between adjacent nodes $i$ and $j$.
Table \ref{tab:transfer_function} display the most widely adopted activation
functions in the design of an ANN, which is used in the current study.

\input{fig/ann_active_function}

\subsection{Weights learning}
The weight training in an ANN is to minimize the error function, such as the
most widely used mean square error function, which calculates the difference
between the desired and the prediction output values averaged overall examples.
Gradient descent algorithm is widely adopted to reduce the value of an error
function, which has been successfully applied in many practical areas. However,
this class of algorithms is plagued by the possible existence of local minima
or ”flat spots” and ”the curse of dimensionality.” One method to overcome this
problem is to adopt a genetic algorithm.


\section{Evolutionary Artificial Neural Network}

\subsection{General neural network}
In this paper, the feedforward neural network(NN) is adopted in the current
study, since it is straightforward and simple to code. For function
approximation through a NN, Cybenko demonstrated that a two-layer perceptron
can form an arbitrarily close approximation to any continuous nonlinear
mapping\cite{cybenko1989approximation}. Therefore, a two-layer feedforward NN
is proposed in the present study. Fig.\ref{fig:gnn} shows a general framework for a
two-layer NN, in which the number of nodes in the hidden layer and the
connection with inputs, are critical in the design of a NN. For nodes in the
hidden layer, we can think of them as feature extractors or detectors.
Therefore, nodes within it should partially be connected with the inputs of NN,
since the unnecessary connections would increase the model's complicacy, which
will reduce a NN’s performance. Because we treat the nodes in the hidden layer
as feature extractors, so the number of nodes in this layer should be less than
the number of inputs. For the nodes in the last layer, every node should be
fully connected with nodes in the previous layer, since we think of the nodes
in the hidden layer as features. The rest, which affects a NN’s performance,
are activation functions, and NN training method.


\input{fig/ann_general_architecture}

\subsection{Transfer function}
The transfer fucntion has been shown to be one of the critical part of the
architecture. Liu \cite{liu1996evolutionary} et al. have claimed that ANNs with
different active functions play a important role in the arthitecture's performance.
A generalized transfer function can be written as

\begin{equation}
	y_i = f_i(\sum_{j=1}^n{w_{ij}x_j - \theta})
\end{equation}

where $y_i$ is the output of the node $i$, $x_j$ is the $j$th input to the node,
and $w_{ij}$ is the connection weight between adjacent nodes $i$ and $j$. Most
widely transfer function  $f_i$ is listed in table \ref{tab:transfer_function}

\input{fig/ann_active_function}

\subsection{Weights learning}
The weight training in ANN is to minimize the error function, such as the most
widely used mean square error which calculate the difference  between the
desired and the prediction output values averaged over all examples.
Backpropation algorithm has been successful applied to in many areas, and it's
based on gradient descent. However, this class of algorithms are  plagued by
the possible existance of local minima or "flat spots" and "the curse of
dimensionality". One method to overcome this problem is to adopt
EANN's\cite{yao1999evolving} through a systematic search process\cite{elsken2019neural} to build
the topology of ANN.








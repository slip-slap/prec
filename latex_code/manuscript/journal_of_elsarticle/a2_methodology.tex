\section{Methodology}
The works involved in the evolution process of ANN can be categorized into
three parts:  search space which defines the architecture can be represented in
principle; search strategy which details how to explore the search space;
performance estimation strategy that refers to the process of estimating this
performance.


\subsection{Search Space}
\input{a0_figure_evolutionary_ann.tex}
we propose a GNN framework as shown in Figure \ref{fig:gnn}. The search space
is parametrized by: (i) the number of nodes m(possibly unbounded) in hidden
layer, to narrow down the search space, the assumptions is that m less than n; (ii) the type of
operation every nodes executes, e.g., sigmoid, linear, gaussian. (iii) the
connection relationship between hidden nodes and inputs; (IV) if a connection
exists, the weight value in the connection.

Therefore, evolution in EANN can be divided into four different levels: topology, learning
rules, active functions, and connection weights. For the evolution of toplogy,
the aim is to find an optimal ANN architecture for a specfic problem. The
architecture of a neural network determines the information processing
capability in application, which is the foundation of the ANN. Two critical
issues are involved in the search process of an ANN architecture: the
representation and the search operators.
Figure \ref{fig:evolution} summarizes different these four levels of evolution in ANN's.

The inputs of the neural network is consist of four parts: in-plane loading
$N_x$, $N_y$, and $N_{xy}$, design parameters of laminate, two distinct fiber
orientation angle $\theta_1$ and $\theta_2$, ply thickness $t$, total number of
plies $N$; five engineering constants of composite materials, $E_1$, $E_2$, ;
five strength parameters of a unidirectional lamina. There are two outputs in
the neural network, safety factors for MS theory and Tsai-Wu theory, respectively.
\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{./a0_figure_ann_for_clt_architecture.png}
\end{figure}

\subsection{Search Strategy}

The classic approach has always adopted binary strings to encode an alternative solutions. 

Tab.\ref{tab:parents1}  gives an example of the binary representation of an ANN
whose architecture is as shown in Fig.\ref{fig:parents1}. Each number in the
digit denotes the connection relationship between input and nodes in hidden
layer. It an connection exists, it's indicated by number one, otherwise, the
number takes zero. The first sixteen digits denotes the connection relationship, and the
last two digits are stand for  the corresponding kernal function. 

First,  random initialize ANN population, partial training every ANN, 
For the evolution of the topology,

\subsection{Performance estimation strategy}
The simplest approach to this problem is to perform a standard training and
validation of the architecture on dataset, however, this method is inefficient
and computational intensive. Therefore, much recent
research\cite{baker2017accelerating} foces on developing methods that reduce
the cost of performance estimation.

\input{a0_figure_ann_for_clt_architecture_example1_table}


